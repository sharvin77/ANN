import matplotlib.pyplot as plt
import numpy as np

class AF():
    def linear(x):
     return x

    def sigmoid(x):
     return 1/(1+np.exp(-x))
    
    def tanh(x):
     return np.tanh(x)
    
    def RELU(x):
        x1=[]
        for i in x:
            if i<0:
                x1.append(0)
            else:
                x1.append(i)
        return x1

    def softmax(x):
     return np.exp(x) / np.sum(np.exp(x), axis=0)


while True:
    n = int(input("Enter\n 0. to Cancel\n 1. Linear\n 2. Sigmoid\n 3.Tanh\n 4.ReLU\n 5.Softmax\n "))
    if n==0:
        break
    if n==1:
        x = np.linspace(-10, 10)
        plt.plot(x, AF.linear(x))
        plt.title('Activation Function :Linear')
        plt.show()
    elif n==2:
        x = np.linspace(-10, 10)
        plt.plot(x, AF.sigmoid(x))
        plt.title('Activation Function :Sigmoid')
        plt.show()
    elif n==3:
        x = np.linspace(-10, 10)
        plt.plot(x, AF.tanh(x))
        plt.title('Activation Function :Tanh')
        plt.show()
    elif n==4:
        x = np.linspace(-10, 10)
        plt.plot(x, AF.RELU(x))
        plt.title('Activation Function :ReLU')
        plt.show()
    elif n==5:
        x = np.linspace(-10, 10)
        plt.plot(x, AF.softmax(x))
        plt.title('Activation Function :Softmax')
        plt.show()